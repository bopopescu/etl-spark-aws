{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Startup Spark locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "# import pyspark\n",
    "# sc = pyspark.SparkContext(appName=\"myAppName\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SQLContext\n",
    "# sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnan, count, when, col, desc, udf, col, sort_array, asc, avg\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load song data\n",
    "\n",
    "For some reason, we can not load all the data at once. the function bellow\n",
    "  read the 30 files at once in each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(l_files):\n",
    "    # NOTE: for some reason, locally I could not load more than 30 files by step\n",
    "    i_step = 30\n",
    "    df = None\n",
    "    for ii in range(0, len(l_files), i_step):\n",
    "        if ii == 0:\n",
    "            continue\n",
    "        df_new = spark.read.json(l_files[ii-i_step:ii])\n",
    "        if isinstance(df, type(None)):\n",
    "            df = df_new\n",
    "        else:\n",
    "            df = df.union(df_new)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total fies: 604\n"
     ]
    }
   ],
   "source": [
    "data = './data/songs/A/*/*.json'\n",
    "output_data = './data/output'\n",
    "l_files = !ls {data}\n",
    "l_files2 = list(l_files)\n",
    "print(f'total fies: {len(l_files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.json(l_files2)\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "CPU times: user 82.8 ms, sys: 19 ms, total: 102 ms\n",
      "Wall time: 3.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = load_data(l_files2)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract columns to create songs table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# songs - songs in music database\n",
    "# song_id, title, artist_id, year, duration\n",
    "\n",
    "df.createOrReplaceTempView(\"staging_songs\")\n",
    "songs_table = spark.sql(\"\"\"\n",
    "    SELECT song_id, title, artist_id, year, duration\n",
    "    FROM staging_songs\n",
    "    ORDER BY song_id\n",
    "\"\"\")\n",
    "\n",
    "songs_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_table = df['song_id', 'title', 'artist_id', 'year', 'duration']\n",
    "songs_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+----+---------+\n",
      "|           song_id|               title|         artist_id|year| duration|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "|SOAFBCP12A8C13CC7D|King Of Scurf (20...|ARTC1LV1187B9A4858|1972|301.40036|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_table.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# write songs table to parquet files partitioned by year and artist\n",
    "s_song_s3_path = f\"s3://{output_data}/songs.parquet\"\n",
    "s_song_s3_path = f\"{output_data}/songs.parquet\"\n",
    "songs_table.write.mode('overwrite').partitionBy('year', 'artist_id').parquet(s_song_s3_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract columns to create artists table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- named_struct(artist_id, artist_id, name, artist_name AS `name`, location, artist_location AS `location`, latitude, artist_latitude AS `latitude`, longitude, artist_longitude AS `longitude`): struct (nullable = false)\n",
      " |    |-- artist_id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- location: string (nullable = true)\n",
      " |    |-- latitude: double (nullable = true)\n",
      " |    |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# artists - artists in music database\n",
    "# artist_id, name, location, latitude, longitude\n",
    "artists_table = spark.sql(\"\"\"\n",
    "    SELECT (\n",
    "        artist_id,\n",
    "        artist_name AS name,\n",
    "        artist_location AS location,\n",
    "        artist_latitude AS latitude,\n",
    "        artist_longitude AS longitude)\n",
    "    FROM staging_songs\n",
    "    ORDER BY artist_id\n",
    "\"\"\")\n",
    "\n",
    "artists_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# artists_table = df['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']\n",
    "artists_table = df.select(\n",
    "    col(\"artist_id\").alias(\"artist_id\"),\n",
    "    col(\"artist_name\").alias(\"name\"),\n",
    "    col(\"artist_location\").alias(\"location\"),\n",
    "    col(\"artist_latitude\").alias(\"latitude\"),\n",
    "    col(\"artist_longitude\").alias(\"longitude\"))\n",
    "artists_table = artists_table.drop_duplicates(subset=['artist_id'])\n",
    "artists_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+--------------------+--------+---------+\n",
      "|         artist_id|               name|            location|latitude|longitude|\n",
      "+------------------+-------------------+--------------------+--------+---------+\n",
      "|AR9ODB41187FB459B2|Organized Konfusion|SPRINGFIELD, Virg...|    null|     null|\n",
      "+------------------+-------------------+--------------------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artists_table.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = './data/logs/*/*/*.json'\n",
    "df = spark.read.json(data)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by actions for song plays\n",
    "df.createOrReplaceTempView(\"staging_events\")\n",
    "df = spark.sql(\"\"\"\n",
    "    SELECT * FROM staging_events\n",
    "    WHERE page='NextSong'\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.page == 'NextSong')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract columns for users table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# users - users in the app\n",
    "# user_id, first_name, last_name, gender, level\n",
    "\n",
    "users_table = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        userId AS user_id,\n",
    "        firstName AS first_name,\n",
    "        lastName AS last_name,\n",
    "        gender AS gender,\n",
    "        level AS level\n",
    "    FROM staging_events\n",
    "    ORDER BY user_id\n",
    "\"\"\")\n",
    "\n",
    "users_table.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_table = df.select(\n",
    "    col(\"userId\").alias(\"user_id\"),\n",
    "    col(\"firstName\").alias(\"first_name\"),\n",
    "    col(\"lastName\").alias(\"last_name\"),\n",
    "    col(\"gender\").alias(\"gender\"),\n",
    "    col(\"level\").alias(\"level\"))\n",
    "users_table = users_table.drop_duplicates(subset=['user_id'])\n",
    "users_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write users table to parquet files\n",
    "s_s3_path = \"{}/users.parquet\".format(output_data)\n",
    "users_table.write.mode('overwrite').parquet(s_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract columns to create time table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|           ts|\n",
      "+-------------+\n",
      "|1542241826796|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gr = spark.sql(\"\"\"\n",
    "    SELECT ts\n",
    "    FROM staging_events\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "# gr.collect()\n",
    "gr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|           ts|\n",
      "+-------------+\n",
      "|1542241826796|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(1).select('ts').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2018 22:30:26\n"
     ]
    }
   ],
   "source": [
    "# testing convertion\n",
    "import time\n",
    "\n",
    "x = 1542241826796\n",
    "\n",
    "s_format = '%m/%d/%Y %H:%M:%S'\n",
    "print(time.strftime(s_format, time.localtime(x/1000.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create timestamp column from original timestamp column\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "get_timestamp = udf(lambda x: time.localtime(x/1000.))\n",
    "get_timestamp = udf(lambda ts: ts/1000)\n",
    "df2 = df.withColumn('timestamp', get_timestamp('ts'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- datetime: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create datetime column from original timestamp column\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "get_datetime = udf(lambda x: time.strftime(\n",
    "    '%Y-%m-%d %H:%M:%S', time.localtime(x/1000.0)), DateType())\n",
    "get_datetime = udf(lambda ts: datetime.fromtimestamp(ts/1000), DateType())\n",
    "df2 = df2.withColumn('datetime', get_datetime('ts'))\n",
    "\n",
    "\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.limit(1).select('datetime').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView(\"staging_events\")\n",
    "\n",
    "gr = spark.sql(\"\"\"\n",
    "    SELECT ts\n",
    "    FROM staging_events\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "# gr.collect()\n",
    "# df.select('datetime').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select('ts').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start_time: date (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# time - timestamps of records in songplays broken down into specific units\n",
    "# start_time, hour, day, week, month, year, weekday\n",
    "from pyspark.sql.functions import (year, month, dayofmonth, hour, weekofyear,\n",
    "                                   date_format, dayofweek)\n",
    "\n",
    "time_table = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        datetime AS start_time,\n",
    "        hour(datetime) AS hour,\n",
    "        dayofmonth(datetime) AS day,\n",
    "        weekofyear(datetime) AS week,\n",
    "        month(datetime) AS month,\n",
    "        year(datetime) AS year,\n",
    "        dayofweek(datetime) AS weekday\n",
    "    FROM staging_events\n",
    "    ORDER BY start_time\n",
    "\"\"\")\n",
    "\n",
    "time_table.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start_time: date (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_table = df2.select(\n",
    "    col('datetime').alias('start_time'),\n",
    "    hour('datetime').alias('hour'),\n",
    "    dayofmonth('datetime').alias('day'),\n",
    "    weekofyear('datetime').alias('week'),\n",
    "    month('datetime').alias('month'),\n",
    "    year('datetime').alias('year'),\n",
    "    dayofweek('datetime').alias('weekday')\n",
    ")\n",
    "time_table = time_table.drop_duplicates(subset=['start_time'])\n",
    "\n",
    "time_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_table.select('hour').limit(1).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_s3_path = \"{}/time.parquet\".format(output_data)\n",
    "# time_table.write.mode('overwrite').partitionBy(\n",
    "#     'year', 'month').parquet(s_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract columns from joined song and log datasets to create songplays table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|           ts|\n",
      "+-------------+\n",
      "|1542241826796|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('ts').limit(1).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- datetime: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in song data to use for songplays table\n",
    "s_s3_path = f\"{output_data}/songs.parquet\"\n",
    "# songs_table = spark.read.parquet(s_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start_time: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- session_id: long (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- user_agent: string (nullable = true)\n",
      " |-- songplay_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE: should've read from parque songs table, but I was uneble to save the data\n",
    "# locally\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# songplays - records in log data associated with song plays i.e. records with page NextSong\n",
    "# songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent\n",
    "\n",
    "# serial like number:\n",
    "# source https://towardsdatascience.com/adding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6\n",
    "# https://stackoverflow.com/questions/53042432/creating-a-row-number-of-each-row-in-pyspark-dataframe-using-row-number-functi\n",
    "\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "\n",
    "window = Window.orderBy(col('song_id'))\n",
    "\n",
    "songplays_table = (df2.join(\n",
    "    songs_table.alias('songs'), df2.song == col('songs.title'))\n",
    "     .select(\n",
    "        col('timestamp').alias('start_time'),\n",
    "        col('userId').alias('user_id'),\n",
    "        col('level').alias('level'),\n",
    "        col('songs.song_id').alias('song_id'),\n",
    "        col('songs.artist_id').alias('artist_id'),\n",
    "        col('sessionId').alias('session_id'),\n",
    "        col('location').alias('location'),\n",
    "        col('userAgent').alias('user_agent'),\n",
    "     )\n",
    "    .withColumn('songplay_id', row_number().over(window))\n",
    ")\n",
    "\n",
    "songplays_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView(\"staging_events\")\n",
    "songs_table.createOrReplaceTempView(\"songs_table\")\n",
    "\n",
    "songplays_table = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        events.timestamp AS start_time,\n",
    "        events.userId AS user_id,\n",
    "        events.level AS level,\n",
    "        songs.song_id AS song_id,\n",
    "        songs.artist_id AS artist_id,\n",
    "        events.sessionId AS session_id,\n",
    "        events.location AS location,\n",
    "        events.userAgent AS user_agent\n",
    "    FROM staging_events AS events\n",
    "    INNER JOIN songs_table AS songs\n",
    "        ON events.song = songs.title\n",
    "        AND events.length = songs.duration\n",
    "    ORDER BY song_id\n",
    "\"\"\")\n",
    "\n",
    "window = Window.orderBy(col('song_id'))\n",
    "songplays_table = songplays_table.withColumn(\n",
    "    'songplay_id', row_number().over(window))\n",
    "\n",
    "songplays_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# songplays_table.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zipline",
   "language": "python",
   "name": "zipline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
